
\chapter{Preliminaries}\label{ch:prelims}
When introducing novel methods throughout this thesis, we make use of several existing algorithms and constructs. In this chapter, we present these relevant preliminaries grouped by the fields they pertain to; estimation and cryptography.

% 
% 8888888888 .d8888b. 88888888888      8888888b.  8888888b.  8888888888 888      8888888 888b     d888 
% 888       d88P  Y88b    888          888   Y88b 888   Y88b 888        888        888   8888b   d8888 
% 888       Y88b.         888          888    888 888    888 888        888        888   88888b.d88888 
% 8888888    "Y888b.      888          888   d88P 888   d88P 8888888    888        888   888Y88888P888 
% 888           "Y88b.    888          8888888P"  8888888P"  888        888        888   888 Y888P 888 
% 888             "888    888          888        888 T88b   888        888        888   888  Y8P  888 
% 888       Y88b  d88P    888          888        888  T88b  888        888        888   888   "   888 
% 8888888888 "Y8888P"     888          888        888   T88b 8888888888 88888888 8888888 888       888 
%                                                                                                      
%                                                                                                      
%                                                                                                      
% 

\section{Estimation Preliminaries}\label{sec:prelims:est_prelims}
Sensor and estimate data that we consider is primarily Bayesian in nature and typically consists of estimates and associated estimate uncertainties. The linear Kalman filter and the linearising extended Kalman filter, along with their information filter equivalents, are particularly useful in the estimation and fusion of such data. A general fusion algorithm, the covariance intersection, used when data cross-correlations are unknown, is also introduced.

% 
% ##    ## ######## 
% ##   ##  ##       
% ##  ##   ##       
% #####    ######   
% ##  ##   ##       
% ##   ##  ##       
% ##    ## ##       
% 

\subsection{Kalman Filter}\label{subsec:prelims:kf}
The Kalman filter (KF) [orig kf] is a popular and well studied recursive state estimation filter that produces estimates and their error covariances $\hat{\vec{x}}_{k|k^\prime} \in \mathbb{R}^n$ and $\mat{P}_{k|k^\prime} \in \mathbb{R}^{n \times n}$, respectively, for a timestep $k \in \mathbb{N}$, given measurements up to and including timestep $k^\prime \in \mathbb{N}$ [kf uses]. Although the KF supports the estimation of a system state which can be manipulated through an external input, this thesis primarily discusses scenarios where no external inputs are known to the estimator and will introduce the filter with these inputs set to $\vec{0}$. In this form, the KF assumes the existence of a true state $\vec{x}_k \in \mathbb{R}^n$ at each timestep $k$, following the linear process model
\begin{equation}\label{eq:prelims:lin_gauss_process_model}
    \vec{x}_k = \mat{F}_k \vec{x}_{k-1} + \vec{w}_k\,,
\end{equation}
where $\vec{w}_k \sim \mathcal{N}(\vec{0}, \mat{Q}_k)$ with known covariance $\mat{Q}_k \in \mathbb{R}^{n \times n}$. Similarly, measurements $\vec{z}_k \in \mathbb{R}^m$ are assumed to follow the linear measurement model
\begin{equation}\label{eq:prelims:lin_gauss_measurement_model}
    \vec{z}_k = \mat{H}_k \vec{x}_k + \vec{v}_k\,,
\end{equation}
where $\vec{v}_k \sim \mathcal{N}(\vec{0}, \mat{R}_k)$ with known covariance $\mat{R}_k \in \mathbb{R}^{m \times m}$. The filter requires initialisation with some known values $\hat{\vec{x}}_{0|0}$ and $\mat{P}_{0|0}$ and is computed recursively in two steps. First, the estimate for the next timestep is predicted without new measurement information, known as the \textit{prediction} step, and is given by
\begin{equation}\label{eq:prelims:kf_est_predict}
    \hat{\vec{x}}_{k|k-1} = \mat{F}_k \hat{\vec{x}}_{k-1|k-1}
\end{equation}
and
\begin{equation}\label{eq:prelims:kf_cov_predict}
    \mat{P}_{k|k-1} = \mat{F}_k \mat{P}_{k-1|k-1} \mat{F}_k^\top + \mat{Q}_k\,.
\end{equation}
Next, this prediction is updated with current measurement information, known as the \textit{update} step, and given by
\begin{equation}\label{eq:prelims:kf_est_update}
    \hat{\vec{x}}_{k|k} = \hat{\vec{x}}_{k|k-1}+\mat{P}_{k|k-1}\mat{H}_k^\top\left(\mat{H}_k\mat{P}_{k|k-1}\mat{H}_k^\top + \mat{R}_k\right)^{-1}\left(z_k-\mat{H}_k\hat{\vec{x}}_{k|k-1}\right)
\end{equation}
and
\begin{equation}\label{eq:prelims:kf_cov_update}
    \mat{P}_{k|k} = \mat{P}_{k|k-1}-\mat{P}_{k|k-1}\mat{H}_k^\top\left(\mat{H}_k\mat{P}_{k|k-1}\mat{H}_k^\top + \mat{R}_k\right)^{-1}\mat{H}\mat{P}_{k|k-1}\,.
\end{equation}
In addition to alternating prediction and update steps as time progresses, the update step \eqref{eq:prelims:kf_est_update} and \eqref{eq:prelims:kf_cov_update} can be skipped at timesteps when no measurements are available. Similarly, when multiple independent measurements are present at the same timestep, the update step can be repeated for each measurement individually. Detailed derivations of the KF and discussions on its properties can be found in [huag+chap].

% 
% ##    ## ########     #######  ########  ######## 
% ##   ##  ##          ##     ## ##     ##    ##    
% ##  ##   ##          ##     ## ##     ##    ##    
% #####    ######      ##     ## ########     ##    
% ##  ##   ##          ##     ## ##           ##    
% ##   ##  ##          ##     ## ##           ##    
% ##    ## ##           #######  ##           ##    
% 

\subsection{Kalman Filter Optimality}\label{subsec:prelims:kf_opt}
One of the reasons for the ubiquity and popularity of the KF introduced in section \ref{subsec:prelims:kf} is its optimality in terms of mean square error (MSE) [huag+chap]. That is, the estimate's error covariances, defined by the expectation capturing mean square error,
\begin{equation}
    \mat{P}_{k|k} = \mathbb{E}\left\{\left(\vec{x}_k - \hat{\vec{x}}_{k|k}\right)\left(\vec{x}_k - \hat{\vec{x}}_{k|k}\right)^\top\right\}\,,
\end{equation}
and computed by \eqref{eq:prelims:kf_cov_predict} and \eqref{eq:prelims:kf_cov_update}, can be shown to equal the theoretical lower bound on the covariance of an unbiased estimator when process and measurement models \eqref{eq:prelims:lin_gauss_process_model} and \eqref{eq:prelims:lin_gauss_measurement_model}, respectively, capture the estimated environment exactly [huag refs for crlb]. This property will be used in later cryptographic discussions in this thesis to guarantee estimator performances in terms of MSE. Further reading on the definitions and proofs of KF optimality can be found in [crlb,huag,etc].

% 
% ######## ##    ## ######## 
% ##       ##   ##  ##       
% ##       ##  ##   ##       
% ######   #####    ######   
% ##       ##  ##   ##       
% ##       ##   ##  ##       
% ######## ##    ## ##       
% 

\subsection{Extended Kalman Filter}\label{subsec:prelims:ekf}
The extended Kalman filter (EKF) is a recursive state estimation filter applicable to non-linear models and closely related to the linear KF [ekf paper,huag+chap]. The filter produces estimates and their covariances at each timestep by linearising models at the current estimate and evaluating the filter similarly to the KF. As in the KF, a true state $\vec{x}_k$ is assumed to follow known models. The process model is now non-linear and given by
\begin{equation}\label{eq:prelims:nonlin_gauss_process_model}
    \vec{x}_k = f_k(\vec{x}_{k-1}) + \vec{w}_k\,,
\end{equation}
where again $\vec{w}_k \sim \mathcal{N}(\vec{0}, \mat{Q}_k)$ with known covariance $\mat{Q}_k$. Similarly, measurements are assumed to follow the non-linear measurement model
\begin{equation}\label{eq:prelims:nonlin_gauss_measurement_model}
    \vec{z}_k = h_k(\vec{x}_k) + \vec{v}_k\,,
\end{equation}
with $\vec{v}_k \sim \mathcal{N}(\vec{0}, \mat{R}_k)$ and known covariance $\mat{R}_k$. The EKF \textit{prediction} step is given by
\begin{equation}\label{eq:prelims:ekf_est_predict}
    \hat{\vec{x}}_{k|k-1} = f_k\left(\hat{\vec{x}}_{k-1|k-1}\right)
\end{equation}
and
\begin{equation}\label{eq:prelims:ekf_cov_predict}
    \mat{P}_{k|k-1} = \hat{\mat{F}}_k \mat{P}_{k-1|k-1} \hat{\mat{F}}_k^\top + \mat{Q}_k\,,
\end{equation}
with Jacobian
\begin{equation}\label{eq:prelims:ekf_predict_jacobian}
    \hat{\mat{F}}_k = \left.\frac{\partial f_k}{\partial \vec{x}}\right|_{\hat{\vec{x}}_{k-1|k-1}}
\end{equation}
linearising the process model at the latest estimate for estimate error covariance prediction. The EKF \textit{update} step is given by
\begin{equation}\label{eq:prelims:ekf_est_update}
    \hat{\vec{x}}_{k|k} = \hat{\vec{x}}_{k|k-1}+\mat{P}_{k|k-1}\hat{\mat{H}}_k^\top\left(\hat{\mat{H}}_k\mat{P}_{k|k-1}\hat{\mat{H}}_k^\top + \mat{R}_k\right)^{-1}\left(z_k-h_k(\hat{\vec{x}}_{k|k-1})\right)
\end{equation}
and
\begin{equation}\label{eq:prelims:ekf_cov_update}
    \mat{P}_{k|k} = \mat{P}_{k|k-1}-\mat{P}_{k|k-1}\hat{\mat{H}}_k^\top\left(\hat{\mat{H}}_k\mat{P}_{k|k-1}\hat{\mat{H}}_k^\top + \mat{R}_k\right)^{-1}\hat{\mat{H}}\mat{P}_{k|k-1}\,,
\end{equation}
with Jacobian
\begin{equation}\label{eq:prelims:ekf_update_jacobian}
    \hat{\mat{H}}_k = \left.\frac{\partial h_k}{\partial \vec{x}}\right|_{\hat{\vec{x}}_{k|k-1}}
\end{equation}
linearising the measurement model. Unlike the linear KF, by linearising the models the EKF propagates Gaussian model noises in its estimates that may not be Gaussian in reality, even when process and measurement models \eqref{eq:prelims:nonlin_gauss_process_model} and \eqref{eq:prelims:nonlin_gauss_measurement_model}, respectively, are exactly correct. For this reason, the EKF does not hold the same guarantees on optimality as the KF does. To a similar effect, highly non-linear models or inaccurate models can lead to greater inaccuracies and divergence of estimates from true states in EKF estimates. Despite these downsides, its scalability and efficiency have made the EKF an industry-standard estimation filter for non-linear systems [smth on uses of ekf]. More details on the EKF and its derivation can be found in [huag+chap].

% 
% #### ######## 
%  ##  ##       
%  ##  ##       
%  ##  ######   
%  ##  ##       
%  ##  ##       
% #### ##       
% 

\subsection{Information Filter}\label{subsec:prelims:if}
The information filter (IF) is an algebraic reformulation of the KF from section \ref{subsec:prelims:kf} [niehsenInformationFusionBased2002, Mutambara, another]. The information form of the filter simplifies the update step of the filter making it more suitable when multiple independent measurements are present at the same timestep. The key difference of the IF is the storing and propagation of the information vector $\hat{\vec{y}}_{k|k'}$ and information matrix $\mat{Y}_{k|k'}$ rather than the estimate and its error covariance, $\hat{\vec{x}}_{k|k'}$ and $\mat{P}_{k|k'}$, respectively, stored by the KF. When assuming the same linear and Gaussian models \eqref{eq:prelims:lin_gauss_process_model} and \eqref{eq:prelims:lin_gauss_measurement_model}, the information vector and matrix are related to the estimate and its covariance by
\begin{equation}\label{eq:prelims:info_vec_defn}
    \hat{\vec{y}}_{k|k'} = \mat{P}_{k|k'}^{-1}\hat{\vec{x}}_{k|k'}
\end{equation}
and
\begin{equation}\label{eq:prelims:info_mat_defn}
    \mat{Y}_{k|k'} = \mat{P}_{k|k'}^{-1}\,,
\end{equation}
for an estimated timestep $k$ and measurements from timesteps up to and including $k'$. The estimation of the information vector and information matrix requires an initialisation of $\hat{\vec{y}}_{0|0}$ and $\mat{Y}_{0|0}$, similarly to the KF, and is also performed by iterating distinct predict and update filter steps. The prediction step is given by
\begin{equation}\label{eq:prelims:if_info_vec_predict}
    \hat{\vec{y}}_{k|k-1} = \mat{Y}_{k|k-1}\mat{F}_k\mat{Y}_{k-1|k-1}^{-1}\hat{\vec{y}}_{k-1|k-1}
\end{equation}
and
\begin{equation}\label{eq:prelims:if_info_mat_predict}
    \mat{Y}_{k|k-1} = \left(\mat{F}_k\mat{Y}_{k-1|k-1}^{-1}\mat{F}_k^\top + \mat{Q}_k\right)^{-1}\,.
\end{equation}
The update step is given by
\begin{equation}\label{eq:prelims:if_info_vec_update_single}
    \hat{\vec{y}}_{k|k} = \hat{\vec{y}}_{k|k-1} + \vec{i}_k
\end{equation}
and
\begin{equation}\label{eq:prelims:if_info_mat_update_single}
    \mat{Y}_{k|k} = \mat{Y}_{k|k-1} + \mat{I}_k\,,
\end{equation}
where added terms $\vec{i}_k$ and $\mat{I}_k$ are known as the measurement vector and measurement matrix, respectively, and are defined as
\begin{equation}
    \vec{i}_k = \mat{H}_k^\top\mat{R}_k^{-1}z_k
\end{equation}
and
\begin{equation}
    \mat{I}_k = \mat{H}_k^\top\mat{R}_k^{-1}\mat{H}_k\,.
\end{equation}

Since all information related to a measurement and its sensor are captured in $\vec{i}_k$ and $\mat{I}_k$, namely the measured value $z_k$, measurement model $\mat{H}_k$ and measurement error $\mat{R}_k$, sequential IF update steps required in the presence of multiple sensors are easily computed as a summation of this information from each sensor. That is, if we consider the same process model \eqref{eq:prelims:lin_gauss_process_model} and multiple sensors $i$, $1\leq i\leq n$, making independent measurements that follow models
\begin{equation}
    \vec{z}_{k,i} = \mat{H}_{k,i} \vec{x}_k + \vec{v}_{k,i}\,,
\end{equation}
with $\vec{v}_{k,i}\sim\mathcal{N}(\vec{0},\mat{R}_{k,i})$ and known covariances $\mat{R}_{k,i}$, the update step of the filter using all measurements at timestep $k$ can be written as
\begin{equation}\label{eq:prelims:if_info_vec_update}
    \hat{\vec{y}}_{k|k} = \hat{\vec{y}}_{k|k-1} + \sum_{i=1}^n\vec{i}_{k,i}
\end{equation}
and
\begin{equation}\label{eq:prelims:if_info_mat_update}
    \mat{Y}_{k|k} = \mat{Y}_{k|k-1} + \sum_{i=1}^n\mat{I}_{k,i}\,,
\end{equation}
where information vectors $\vec{i}_{k,i}$ and information matrices $\mat{I}_{k,i}$ are now dependent on sensor $i$ and given by
\begin{equation}\label{eq:prelims:if_measurement_vec_defn}
    \vec{i}_{k,i} = \mat{H}_{k,i}^\top\mat{R}_{k,i}^{-1}z_{k,i}
\end{equation}
and
\begin{equation}\label{eq:prelims:if_measurement_mat_defn}
    \mat{I}_{k,i} = \mat{H}_{k,i}^\top\mat{R}_{k,i}^{-1}\mat{H}_{k,i}\,.
\end{equation}
The easily computed summation has led to the IF being particularly suited to distributed estimation environments, where multiple sensors are present and communicational costs need to be reduced [if egs]. In addition, since the IF is strictly a rearrangement of terms in the KF, it holds the same optimality properties as the KF, described in section \ref{subsec:prelims:kf_opt}. For additional reading on the IF, see [mutambara+chap].

% 
% ######## #### ######## 
% ##        ##  ##       
% ##        ##  ##       
% ######    ##  ######   
% ##        ##  ##       
% ##        ##  ##       
% ######## #### ##       
% 

\subsection{Extended Information Filter}\label{subsec:prelims:eif}
The extended information filter (EIF) is an algebraic reformulation of the EKF and represents a non-linear model extension to the linear IF [thrun, another]. As with the IF, its simplification of the update step to a trivial sum has led to its adoption in suitable distributed environments with multiple sensors and a desire to reduce communicational costs [tobias garritsen thesis]. Similarly, the EIF estimates and propagates the information vector $\hat{\vec{y}}_{k|k'}$ and information matrix $\mat{Y}_{k|k'}$ that relate to the state estimate and its covariance by \eqref{eq:prelims:info_vec_defn} and \eqref{eq:prelims:info_mat_defn}, respectively. Assuming the non-linear process model \eqref{eq:prelims:nonlin_gauss_process_model} and multiple sensors $i$, $1\leq i \leq n$, making independent non-linear measurements that follow models
\begin{equation}
    \vec{z}_{k,i} = h_{k,i}\left(\vec{x}_k\right) + \vec{v}_{k,i}\,,
\end{equation}
with $\vec{v}_{k,i}\sim\mathcal{N}(\vec{0},\mat{R}_{k,i})$ and known covariances $\mat{R}_{k,i}$, the EIF predict step is given by
\begin{equation}\label{eq:prelims:eif_info_vec_predict}
    \hat{\vec{y}}_{k|k-1} = \mat{Y}_{k|k-1}f_k\left(\mat{Y}_{k-1|k-1}^{-1}\hat{\vec{y}}_{k-1|k-1}\right)
\end{equation}
and
\begin{equation}\label{eq:prelims:eif_info_mat_predict}
    \mat{Y}_{k|k-1} = \left(\hat{\mat{F}}_k\mat{Y}_{k-1|k-1}^{-1}\hat{\mat{F}}_k^\top + \mat{Q}_k\right)^{-1}\,,
\end{equation}
with Jacobian linearising the process model
\begin{equation}
    \hat{\mat{F}}_k = \left.\frac{\partial f_k}{\partial \vec{x}}\right|_{\hat{\vec{x}}_{k-1|k-1}}\,.
\end{equation}
The update step of the filter using all $n$ measurements is given by
\begin{equation}\label{eq:prelims:eif_info_vec_update}
    \hat{\vec{y}}_{k|k} = \hat{\vec{y}}_{k|k-1} + \sum_{i=1}^n\vec{i}_{k,i}
\end{equation}
and
\begin{equation}\label{eq:prelims:eif_info_mat_update}
    \mat{Y}_{k|k} = \mat{Y}_{k|k-1} + \sum_{i=1}^n\mat{I}_{k,i}\,,
\end{equation}
where information vectors $\vec{i}_{k,i}$ and information matrices $\mat{I}_{k,i}$ now linearise the measurement model and are given by
\begin{equation}\label{eq:prelims:eif_measurement_vec_defn}
    \vec{i}_{k,i} = \hat{\mat{H}}_{k,i}^\top\mat{R}_{k,i}^{-1}\left(z_{k,i} - h_{k,i}\left(\mat{Y}_{k|k-1}^{-1}\hat{\vec{y}}_{k|k-1}\right) + \hat{\mat{H}}_{k,i}\mat{Y}_{k|k-1}^{-1}\hat{\vec{y}}_{k|k-1}\right)
\end{equation}
and
\begin{equation}\label{eq:prelims:eif_measurement_mat_defn}
    \mat{I}_{k,i} = \hat{\mat{H}}_{k,i}^\top\mat{R}_{k,i}^{-1}\hat{\mat{H}}_{k,i}\,,
\end{equation}
with Jacobian
\begin{equation}
    \hat{\mat{H}}_{k,i} = \left.\frac{\partial h_{k,i}}{\partial \vec{x}}\right|_{\hat{\vec{x}}_{k|k-1}}\,.
\end{equation}
Similarly to the EKF, the linearisation of models leads to estimation errors making optimality guarantees of the KF and IF not hold for the EIF. For further reading and applications of the EIF, see [refs].

% 
% ########  ######  #### 
% ##       ##    ##  ##  
% ##       ##        ##  
% ######   ##        ##  
% ##       ##        ##  
% ##       ##    ##  ##  
% ##        ######  #### 
% 

\subsection{Covariance Intersection and Fast Covariance Intersection}\label{subsec:prelims:fci}
In the filters presented in the previous sections, measurements from the same timestep must be independent for sequential update steps to fuse them correctly. That is, non-zero cross-correlations between measurements can lead to overly confident estimate error covariances and estimation track divergence [crosscor paper]. In some cases, this independence of measurements cannot be guaranteed and a conservative fusion method is required to obtain a final estimate and its error covariance using all available measurements. A typical scenario is the fusion of local estimator estimates themselves, such as those produced by separate KF, IF, EKF or EIF instances, that may contain cross-correlations due to shared process model assumptions during estimation [ci and dependence ref]. Covariance intersection (CI), is a fusion algorithm that fuses estimates or measurements and their error covariances when cross-correlations are unknown [julierNondivergentEstimation] and produces estimates that are guaranteed to be conservative, that is, not overly confident in the error of resulting fused estimates. CI is particularly well suited to fusing estimates $\hat{\vec{x}}_i$ and covariances $\mat{P}_i$ of sensor $i$ in the information form, as given in \eqref{eq:prelims:info_vec_defn} and \eqref{eq:prelims:info_mat_defn}, such that the fusion of $n$ information vectors $\mat{P}_i^{-1}\hat{\vec{x}}_i$, $1\leq i\leq n$, and information matrices $\mat{P}_i^{-1}$, $1\leq i\leq n$, produces a fused information vector $\mat{P}_{\mathsf{fus}}^{-1}\hat{\vec{x}}_{\mathsf{fus}}$ and matrix $\mat{P}_{\mathsf{fus}}^{-1}$, by
\begin{equation}\label{eq:prelims:ci_est}
    \mat{P}_{\mathsf{fus}}^{-1}\hat{\vec{x}}_{\mathsf{fus}} = \sum_{i=1}^n \omega_i \mat{P}_i^{-1} \hat{\vec{x}}_i
\end{equation}
and
\begin{equation}\label{eq:prelims:ci_cov}
\mat{P}_{\mathsf{fus}}^{-1}=\sum_{i=1}^{n}\omega_i \mat{P}_i^{-1}\,,
\end{equation}
for some weights $0\leq \omega_i \leq 1$, $1\leq i \leq n$, such that
\begin{equation}\label{eq:prelims:ci_weight_sum}
    \sum_{i=1}^n \omega_i = 1\,.
\end{equation}
The weights $\omega_i$ are chosen in a way to speed up the convergence of estimate errors over time and to minimise fusion estimate error by minimising some property of the fused estimate error covariance $\mat{P}_{\mathsf{fus}}$. A common choice is to minimise the trace of the covariance [ci example with trace], requiring a solution to
\begin{equation}\label{eq:prelims:ci_min_trace}
    \argmin_{\omega_1,\dots,\omega_n} \left\{\tr\left(\mat{P}_{\mathsf{fus}}\right)\right\}\,.
 \end{equation}
However, minimising the non-linear cost function \eqref{eq:prelims:ci_min_trace} can be computationally costly and has led to the development of faster approximation techniques. The Fast Covariance Intersection (FCI) algorithm [niehsenInformationFusionBased] is one such method, that approximates \eqref{eq:prelims:ci_min_trace} non-iteratively, while still guaranteeing consistency. It is defined by adding new constraints
\begin{equation}\label{eq:prelims:fci_added_constraints}
   \omega_i \tr(\mat{P}_i) - \omega_{i+1} \tr(\mat{P}_{i+1}) = 0\,,
\end{equation}
for $1\leq i\leq n-1$, leading to the linear problem
\begin{equation}\label{eq:prelims:fci_matrix_equation}
    \begin{bmatrix}
        \mathcal{P}_1 & -\mathcal{P}_2 & 0 & \cdots & 0 \\
        0 & \mathcal{P}_2 & -\mathcal{P}_3 & \ddots & \vdots \\
        \vdots & \ddots & \ddots & \ddots & 0 \\
        0 & \cdots & 0 & \mathcal{P}_{n-1} & -\mathcal{P}_n \\
        1 & \cdots & 1 & 1 & 1
    \end{bmatrix}
    \begin{bmatrix}
        \omega_1 \\
        \vdots \\
        \omega_{n-1} \\
        \omega_n
    \end{bmatrix}
    =
    \begin{bmatrix}
        0 \\
        \vdots \\
        0 \\
        1
    \end{bmatrix}
\end{equation}
and its solution
\begin{equation}\label{eq:prelims:fci_solution}
    \omega_i = \frac{\frac{1}{\mathcal{P}_i}}{\sum_{j=1}^n \frac{1}{\mathcal{P}_j}}\,,
\end{equation}
for $1\leq i\leq n$, where $\mathcal{P}_i = \tr(\mat{P}_i)$. Yeilding similar results to the optimal CI \eqref{eq:prelims:ci_min_trace}, FCI has become a popular alternative due to its computational simplicity [uses of fci]. For further reading, and uses of CI and FCI, see [ci refs].

% 
%  .d8888b.  8888888888 .d8888b.       8888888b.  8888888b.  8888888888 888      8888888 888b     d888 
% d88P  Y88b 888       d88P  Y88b      888   Y88b 888   Y88b 888        888        888   8888b   d8888 
% Y88b.      888       888    888      888    888 888    888 888        888        888   88888b.d88888 
%  "Y888b.   8888888   888             888   d88P 888   d88P 8888888    888        888   888Y88888P888 
%     "Y88b. 888       888             8888888P"  8888888P"  888        888        888   888 Y888P 888 
%       "888 888       888    888      888        888 T88b   888        888        888   888  Y8P  888 
% Y88b  d88P 888       Y88b  d88P      888        888  T88b  888        888        888   888   "   888 
%  "Y8888P"  8888888888 "Y8888P"       888        888   T88b 8888888888 88888888 8888888 888       888 
%                                                                                                      
%                                                                                                      
%                                                                                                      
% 

\section{Encryption Preliminaries}\label{sec:prelims:crypto_prelims}
Used cryptographic notions and schemes are summarised here. In addition, the encoding of floating point numbers to integers suitable for encryption by the presented schemes is introduced as well.

% 
% ##    ##  #######  ######## ####  #######  ##    ##  ######  
% ###   ## ##     ##    ##     ##  ##     ## ###   ## ##    ## 
% ####  ## ##     ##    ##     ##  ##     ## ####  ## ##       
% ## ## ## ##     ##    ##     ##  ##     ## ## ## ##  ######  
% ##  #### ##     ##    ##     ##  ##     ## ##  ####       ## 
% ##   ### ##     ##    ##     ##  ##     ## ##   ### ##    ## 
% ##    ##  #######     ##    ####  #######  ##    ##  ######  
% 

\subsection{Meeting Cryptographic Notions}\label{subsec:prelims:crypto_notions}
Cryptographic notions are formal mathematical constructs used to prove the security of cryptographic schemes. A cryptographic notion applies to a type of cryptographic scheme, typically defined by a tuple of algorithms, \textit{e.g.} $(\mathsf{Generate},\mathsf{Encrypt},\mathsf{Decrypt})$, and captures the capabilities of considered attackers and accepted leakage of information to these attackers, that is, what an attacker can do and what they can learn [katzIntroductionModernCryptography2008]. When a scheme of an appropriate form is proved to meet a cryptographic notion it is mathematically guaranteed that any attacker with the capabilities specified by the notion is limited in gaining information from encryptions as also specified by the notion.

Notion capabilities and leakages are dependent on the goal of a particular encryption scheme and are typically defined as a \textit{cryptographic game} (or \textit{experiment}) involving an adversary and the encryption scheme algorithms []. Proofs that schemes meet these notions are often based on existing proofs or assumptions believed to hold, and proved by contrapositive. Below, some cryptographic notions relevant to this thesis are introduced.
\begin{description}
    \item[Indistinguishability under a Chosen Plaintext Attack (IND-CPA)] This notion targets encryption schemes of the form $(\mathsf{Generate},\mathsf{Encrypt},\mathsf{Decrypt})$ and states that an attacker cannot distinguish between encryptions of unknown messages when they can encrypt chosen messages of their own. For detailed definitions and the formal cryptographic game, see [].
    \item[Aggregator Obliviousness (AO)] The AO notion considers an environment of multiple participants where a single \textit{aggregator} computes the summation of input from all other participants. An encryption scheme for this interaction is of the form $(\mathsf{Setup},\mathsf{Encrypt},\mathsf{AggregateDecrypt})$. The notion states that no subset of colluding participants with access to all encryptions and the final summation can compute any more than a party with access to only the colluding participant inputs and the final summation can. The notion of AO and the relevant game are given in [shiPrivacyPreservingAggregationTimeSeries2011].
    \item[Indistinguishability under an Ordered Chosen Plaintext Attack (IND-OCPA)] IND-OCPA is the ideal notion of security for order-revealing encryption, stating that no attacker can distinguish between two sequences of encryptions when the numerical order of messages in the sequences is identical. The associated encryption scheme for the notion is of the form $(\mathsf{Setup},\mathsf{Encrypt},\mathsf{Compare})$. Additional details and the cryptographic game are given in [boldyreva order-preserving symmetric encryption].
\end{description}
As novel encryption schemes and cryptographic games are presented in this thesis, additional information on creating and proving cryptographic notions may be beneficial. For introductory methods and the structure of proofs, see [], while more advanced proofs can be found in [].

% 
% ########  ##     ## ######## 
% ##     ## ##     ## ##       
% ##     ## ##     ## ##       
% ########  ######### ######   
% ##        ##     ## ##       
% ##        ##     ## ##       
% ##        ##     ## ######## 
% 

\subsection{Paillier Homomorphic Encryption Scheme}\label{subsec:prelims:paillier}
The Paillier encryption scheme [paillierPublicKeyCryptosystemsBased1999] is an additively homomorphic encryption scheme that bases its security on the decisional composite residuosity assumption (DCRA) and meets the security notion of IND-CPA. Key generation of the Paillier scheme is performed by choosing two sufficiently large primes of an equal bit length, $p$ and $q$, and computing $N=pq$ [katzIntroductionModernCryptography2008]. The public key is defined by $\mathsf{pk}=N$ and the secret key by $\mathsf{sk}=(p, q)$.

Encryption of a plaintext message $a \in \mathbb{Z}_N$, producing ciphertext $c \in \mathbb{Z}^{*}_{N^2}$, is computed by
\begin{equation}
    c=\mathcal{E}_{\mathsf{pk}}(a) = (N+1)^a \rho^N \pmod{N^2}
\end{equation}
for a randomly chosen $\rho \in \mathbb{Z}_{N}$. Here, $\rho^N$ can be considered the noise term which hides the value $(N+1)^a \pmod{N^2}$, which due to the scheme construction, is an easily computable discrete logarithm. The decryption of the ciphertext $c$ is computed by
\begin{equation}
    a=\mathcal{D}_{\mathsf{pk},\mathsf{sk}}(c) = \frac{L(c^\lambda\pmod{N^2})}{L((N+1)^\lambda\pmod{N^2})} \pmod{N}
\end{equation}
where $\lambda = \mathsf{lcm}(p-1, q-1)$ and $L(\psi) = \frac{\psi-1}{N}$.

In addition to encryption and decryption, the Paillier scheme provides the following homomorphic properties, $\forall a_1,a_2 \in \mathbb{Z}_N$,
\begin{align}
    \mathcal{D}_{\mathsf{pk},\mathsf{sk}}\left(\mathcal{E}_{\mathsf{pk}}(a_1)\mathcal{E}_{\mathsf{pk}}(a_2) \pmod{N^2}\right) &= a_1+a_2 \pmod{N}\,, \label{eq:prelims:paillier_hom_add}\\
    \mathcal{D}_{\mathsf{pk},\mathsf{sk}}\left(\mathcal{E}_{\mathsf{pk}}(a_1)(N+1)^{a_2} \pmod{N^2}\right) &= a_1+a_2 \pmod{N}\,, \label{eq:prelims:paillier_hom_add_plain}\\
    \mathcal{D}_{\mathsf{pk},\mathsf{sk}}\left(\mathcal{E}_{\mathsf{pk}}(a_1)^{a_2} \pmod{N^2}\right) &= a_1a_2 \pmod{N}\,. \label{eq:prelims:paillier_hom_mult}
\end{align}
To simplify this notation, the shorthand operators $\oplus$ and $\otimes$ are used to denote homomorphic addition and multiplication, respectively, as
\begin{equation}\label{eq:prelims:paillier_hom_add_op}
    \mathcal{E}_{\mathsf{pk}}(a_1)\oplus\mathcal{E}_{\mathsf{pk}}(a_2) \equiv \mathcal{E}_{\mathsf{pk}}(a_1)\mathcal{E}_{\mathsf{pk}}(a_2) \pmod{N^2}
\end{equation}
and
\begin{equation}
    a_2\otimes\mathcal{E}_{\mathsf{pk}}(a_1) \equiv \mathcal{E}_{\mathsf{pk}}(a_1)^{a_2} \pmod{N^2}\,.
\end{equation}

% 
%    ###     ######    ######   
%   ## ##   ##    ##  ##    ##  
%  ##   ##  ##        ##        
% ##     ## ##   #### ##   #### 
% ######### ##    ##  ##    ##  
% ##     ## ##    ##  ##    ##  
% ##     ##  ######    ######   
% 

\subsection{Joye-Libert Aggregation Scheme}\label{subsec:prelims:joye_libert_agg}
The Joye-Libert privacy-preserving aggregation scheme [joyeScalableSchemePrivacyPreserving2013] is a scheme defined for $n+1$ parties, where $n$ participants have their data aggregated by an aggregator that performs the homomorphic summation. It is defined on time-series data, indexed by timestep $k$, and meets the security notion AO. Similarly to the Paillier scheme in section \ref{subsec:prelims:paillier}, it bases its security on the DCRA, however, an aggregation scheme generates secret keys for individual participants and the aggregating party, requiring an additional trusted party to perform an initial key generation and distribution step.

Key generation is computed by choosing two equal-length and sufficiently large primes $p$ and $q$, and computing $N=pq$. A hash function $H:\mathbb{Z} \rightarrow \mathbb{Z}_{N^2}^*$ is defined and the scheme's public key is set to $\mathsf{pk}_{\mathsf{A}}=(N, H)$. $n$ secret keys are generated by choosing $\mathsf{sk}_{\mathsf{A}, i}$, $1\leq i\leq n$, uniformly from $\mathbb{Z}_{N^2}$ and distributing them to $n$ participants (whose data is to be aggregated). The last secret key is set as
\begin{equation}
    \mathsf{sk}_{\mathsf{A},0} = -\sum^{n}_{i=1}\mathsf{sk}_{\mathsf{A},i}\,,
\end{equation}
and sent to the aggregator.

At any timestep $k$, a participant $i$ can encrypt time-series plaintext data $a^{(k)}_{i} \in \mathbb{Z}_N$ to ciphertext $c^{(k)}_{i} \in \mathbb{Z}_{N^2}$ with
\begin{equation}
    c^{(k)}_{i} = \mathcal{E}_{\mathsf{pk}_{\mathsf{A}},\mathsf{sk}_{\mathsf{A},i}}\left(a^{(k)}_{i}\right) = (N+1)^{a^{(k)}_{i}} H(k)^{\mathsf{sk}_i} \pmod{N^2}\,.
\end{equation}
Here, we can consider $H(k)^{\mathsf{sk}_i}$ the noise term which hides the easily computable discrete logarithm $(N+1)^{a^{(k)}_{i}} \pmod{N^2}$.

When all encryptions $c^{(k)}_{i}$, $1\leq i\leq n$ are sent to the aggregator, summation and decryption of the aggregated sum are computed by
\begin{equation}\label{eq:prelims:joye_libert_agg_sum}
    c^{(k)} = \prod^{n}_{i=1}c^{(k)}_{i} \pmod{N^2} 
\end{equation}
and
\begin{equation}\label{eq:prelims:joye_libert_agg_dec}
    \sum^{n}_{i=1}a^{(k)}_{i} = \mathcal{D}_{\mathsf{pk}_{\mathsf{A}},\mathsf{sk}_{\mathsf{A},0}}\left(c^{(k)}\right) = \frac{H(k)^{\mathsf{sk}_{\mathsf{A},0}}c^{(k)}-1}{N} \pmod{N}\,.
\end{equation}
Correctness follows from $\sum^{n}_{i=0}\mathsf{sk}_{\mathsf{A},i} = 0$, and thus
\begin{equation*}
    \begin{split}
        &H(k)^{\mathsf{sk}_{\mathsf{A},0}}\prod^{n}_{i=1}c^{(k)}_{i} \pmod{N^2} \\
        \equiv &H(k)^{\mathsf{sk}_{\mathsf{A},0}}\prod^{n}_{i=1}(N+1)^{a^{(k)}_{i}} H(k)^{\mathsf{sk}_{\mathsf{A},i}} \pmod{N^2} \\
        \equiv &H(k)^{\sum^n_{j=0}\mathsf{sk}_{\mathsf{A},j}} \prod^{n}_{i=1}(N+1)^{a^{(k)}_{i}} \pmod{N^2} \\
        \equiv &(N+1)^{\sum^n_{i=1}a^{(k)}_{i}} \pmod{N^2}\,,
    \end{split}
\end{equation*}
removing all noise terms.

% 
%  #######  ########  ########  ######## ########  
% ##     ## ##     ## ##     ## ##       ##     ## 
% ##     ## ##     ## ##     ## ##       ##     ## 
% ##     ## ########  ##     ## ######   ########  
% ##     ## ##   ##   ##     ## ##       ##   ##   
% ##     ## ##    ##  ##     ## ##       ##    ##  
%  #######  ##     ## ########  ######## ##     ## 
% 

\subsection{Lewi Order-Revealing Encryption Scheme}\label{subsec:prelims:lewi_ore}
The Lewi order-revealing encryption (ORE) scheme is a symmetric encryption scheme that allows for message values to be compared numerically after encryption. The scheme does not meet the ideal notion of security for order-revealing encryption, IND-OCPA, due to the inherent difficulty of this problem [chenettePracticalOrderRevealingEncryption2016] but rather meets a weaker simulation-based security given in [chenettePracticalOrderRevealingEncryption2016] that allows for some leakage.

To allow additional control over which encrypted values can be compared, the scheme provides two encryption functions, namely a \textit{left} encryption and \textit{right} encryption, such that comparisons can only take place between left and right encryptions. As complete equations for the scheme are lengthy and unnecessary for following this thesis, only notation will be introduced here. The two encryption equations provided can encrypt plaintexts $a_1,a_2\in\mathbb{Z}$ with a secret key $\mathsf{sk}_{\mathsf{o}}$ and functions
\begin{equation}\label{eq:prelims:lewi_enc}
    \mathcal{E}^{\mathsf{L}}_{\mathsf{sk}_{\mathsf{o}}}\left(a_1\right)\text{ and }\mathcal{E}^{\mathsf{R}}_{\mathsf{sk}_{\mathsf{o}}}\left(a_2\right)
\end{equation}
denoting left and right encryption, respectively. Their comparison can be computed with a function
\begin{equation}\label{eq:prelims:lewi_comp}
    \mathcal{C}\left(\mathcal{E}^{\mathsf{L}}_{\mathsf{sk}_{\mathsf{o}}}(a_1), \mathcal{E}^{\mathsf{R}}_{\mathsf{sk}_{\mathsf{o}}}(a_2)\right) = \mathsf{cmp}(a_1, a_2)\,,
\end{equation}
where
\begin{equation*}
    \mathsf{cmp}(a_1, a_2)=
    \begin{cases}
        -1 & a_1 < a_2\\
        0 & a_1 = a_2\\
        1 & a_1 > a_2
    \end{cases}\,.
\end{equation*}
For details on implementation, see [lewi order revealing].

% 
% ######## ##    ##  ######  
% ##       ###   ## ##    ## 
% ##       ####  ## ##       
% ######   ## ## ## ##       
% ##       ##  #### ##       
% ##       ##   ### ##    ## 
% ######## ##    ##  ######  
% 

\subsection{Encoding Numbers for Encryption}\label{subsec:prelims:encoding}
The Paillier encryption scheme and the Joye-Libert aggregation scheme in sections \ref{subsec:prelims:paillier} and \ref{subsec:prelims:joye_libert_agg}, respectively, both provide encryption on integer inputs in the modulo group $\mathbb{Z}_N$ given a large $N$. In addition, they provide homomorphic operations on these inputs after encryption, namely, the Paillier scheme provides addition and scalar multiplication with \eqref{eq:prelims:paillier_hom_add}, \eqref{eq:prelims:paillier_hom_add_plain} and \eqref{eq:prelims:paillier_hom_mult} while the Joye-Libert scheme provides addition with \eqref{eq:prelims:joye_libert_agg_sum}. For this reason, real-valued estimation variables that may require encryption with these schemes, such as those introduced in section \ref{sec:prelims:est_prelims}, require quantisation and integer mapping such that the operations are preserved after encryption. Throughout this thesis, we will rely on a generalised Q number encoding [oberstarFixedPointRepresentationFractional2007], applying scalar encoding and encryption operations elementwise on multidimensional data, due to applicability and implementation simplicity.

Quantisation to a subset of rational numbers is performed in terms of an output range $M \in \mathbb{N}$ and fractional precision $\phi \in \mathbb{N}$. This contrasts with the common definition in terms of total bits and fractional bits [oberstarFixedPointRepresentationFractional2007, schulzedarupEncryptedCooperativeControl2019, farokhiSecurePrivateControl2017] but allows for a direct mapping to integer ranges which are not a power of two, such as the group $\mathbb{Z}_N$. This rational subset, $\mathbb{Q}_{M,\phi}$, is defined by
\begin{equation}
    \mathbb{Q}_{M,\phi} = \left\{o \,\middle|\, \phi o \in \mathbb{N} \wedge -\left\lfloor\frac{M}{2}\right\rfloor \leq \phi o < \left\lfloor\frac{M}{2}\right\rfloor \right\}\,,
\end{equation}
and can be used to quantise any real number $a \in \mathbb{R}$ by taking the nearest rational $o \in \mathbb{Q}_{M,\phi}$, that is, $\argmin_{o\in\mathbb{Q}_{M,\phi}} |a-o|$. Mapping the rationals $\mathbb{Q}_{M,\phi}$, both positive and negative, to a group $\mathbb{Z}_M$ can then be achieved by modulo arithmetic. Additionally, we note that the Q number format requires a precision factor $\phi$ to be removed after each encoded multiplication. This is captured by a third parameter $d$; the number of additional precision factors present in encodings.

The function for \textit{combined} quantisation and encoding, $\mathsf{E}_{M,\phi,d}(a)$, of a given number $a \in \mathbb{R}$ and with output integer range $\mathbb{Z}_M$, precision $\phi$ and scaling for $d$ prior encoded multiplications, is given by
\begin{equation}\label{eq:prelims:encoding_encode}
    \mathsf{E}_{M,\phi,d}(a) = \left\lfloor \phi^{d+1} a \right\rceil \pmod{M}\,.
\end{equation}
Decoding of an integer $u \in \mathbb{Z}_M$ is given by
\begin{equation}\label{eq:prelims:encoding_decode}
    \mathsf{E}^{-1}_{M,\phi,d}(u)=
    \begin{dcases}
        \frac{u \pmod{M}}{\phi^{d+1}}, & u \pmod{M} \leq \left\lfloor\frac{M}{2}\right\rfloor \\
        -\frac{M - u \pmod{M}}{\phi^{d+1}}, & \text{otherwise}
    \end{dcases}.
\end{equation}

Encoding with \eqref{eq:prelims:encoding_encode} additionally provides two useful properties when used with homomorphic operations,
\begin{equation}\label{eq:prelims:encoding_add}
    \mathsf{E}_{M,\phi,d}(a_1) + \mathsf{E}_{M,\phi,d}(a_2) \pmod{M} \approx \mathsf{E}_{M,\phi,d}(a_1+a_2)
\end{equation}
and
\begin{equation}\label{eq:prelims:encoding_mult}
        \mathsf{E}_{M,\phi,d}(a_1)\mathsf{E}_{M,\phi,d}(a_2) \pmod{M} \approx \mathsf{E}_{M,\phi,d+1}(a_1a_2)\,,
\end{equation}
where deviation from equality stems from quantisation and its associated error
\begin{equation}
    \left|\mathsf{E}^{-1}_{M,\phi,d}\left(\mathsf{E}_{M,\phi,d}(a)\right) - a\right| \leq \frac{1}{\phi}\,.
\end{equation}

In general, choosing a large precision parameter $\phi$ reduces quantisation errors but risks overflow after too many multiplications. When the total number of encoded multiplications, $d_{\mathsf{max}}$, and the largest value to be encoded, $a_{\mathsf{max}}$, are known, $\phi$ can be chosen to avoid overflow by satisfying
\begin{equation}\label{eq:prelims:encoding_overflow_limit}
    \left|\phi^{d_{\mathsf{max}}+1}a_{\mathsf{max}}\right| < \left\lfloor \frac{M}{2} \right\rfloor\,.
\end{equation}
In practice, we set $M=N$ to use the Paillier or Joye-Libert schemes, such that the properties \eqref{eq:prelims:encoding_add} and \eqref{eq:prelims:encoding_mult} can be used with the homomorphic operations of the schemes. Equation \eqref{eq:prelims:encoding_overflow_limit} can then be ignored as $N$ is typically very large ($N>2^{1024}$) and $\phi$ can be made sufficiently large to make quantisation errors negligible.