
\chapter{Preliminaries}\label{ch:prelims}
When introducing novel methods throughout this thesis, we make use of several existing algorithms and constructs. In this chapter, we present these relevant preliminaries grouped by the fields they pertain to; estimation and cryptography.

% 
% 8888888888 .d8888b. 88888888888      8888888b.  8888888b.  8888888888 888      8888888 888b     d888 
% 888       d88P  Y88b    888          888   Y88b 888   Y88b 888        888        888   8888b   d8888 
% 888       Y88b.         888          888    888 888    888 888        888        888   88888b.d88888 
% 8888888    "Y888b.      888          888   d88P 888   d88P 8888888    888        888   888Y88888P888 
% 888           "Y88b.    888          8888888P"  8888888P"  888        888        888   888 Y888P 888 
% 888             "888    888          888        888 T88b   888        888        888   888  Y8P  888 
% 888       Y88b  d88P    888          888        888  T88b  888        888        888   888   "   888 
% 8888888888 "Y8888P"     888          888        888   T88b 8888888888 88888888 8888888 888       888 
%                                                                                                      
%                                                                                                      
%                                                                                                      
% 

\section{Estimation Preliminaries}\label{sec:prelims:est_prelims}
Sensor and estimate data that we consider is primarily Bayesian in nature and typically consists of estimates and associated estimate uncertainties. The linear Kalman filter and the linearising extended Kalman filter, along with their information filter equivalents, are particularly useful in the estimation and fusion of such data. A general fusion algorithm, the covariance intersection, used when data cross-correlations are unknown, is also introduced.

% 
% ##    ## ######## 
% ##   ##  ##       
% ##  ##   ##       
% #####    ######   
% ##  ##   ##       
% ##   ##  ##       
% ##    ## ##       
% 

\subsection{Kalman Filter}\label{subsec:prelims:kf}
The Kalman filter (KF) [orig kf] is a popular and well studied recursive state estimation filter that produces estimates and their error covariances $\hat{\vec{x}}_{k|k^\prime} \in \mathbb{R}^n$ and $\mat{P}_{k|k^\prime} \in \mathbb{R}^{n \times n}$, respectively, for a timestep $k \in \mathbb{N}$, given measurements up to and including timestep $k^\prime \in \mathbb{N}$ [kf uses]. Although the KF supports the estimation of a system state which can be manipulated through an external input, this thesis primarily discusses scenarios where no external inputs are known to the estimator and will introduce the filter with these inputs set to $\vec{0}$. In this form, the KF assumes the existence of a true state $\vec{x}_k \in \mathbb{R}^n$ at each timestep $k$, following the linear process model
\begin{equation}\label{eq:prelims:lin_gauss_process_model}
    \vec{x}_k = \mat{F}_k \vec{x}_{k-1} + \vec{w}_k\,,
\end{equation}
where $\vec{w}_k \sim \mathcal{N}(\vec{0}, \mat{Q}_k)$ with known covariance $\mat{Q}_k \in \mathbb{R}^{n \times n}$. Similarly, measurements $\vec{z}_k \in \mathbb{R}^m$ are assumed to follow the linear measurement model
\begin{equation}\label{eq:prelims:lin_gauss_measurement_model}
    \vec{z}_k = \mat{H}_k \vec{x}_k + \vec{v}_k\,,
\end{equation}
where $\vec{v}_k \sim \mathcal{N}(\vec{0}, \mat{R}_k)$ with known covariance $\mat{R}_k \in \mathbb{R}^{m \times m}$. The filter requires initialisation with some known values $\hat{\vec{x}}_{0|0}$ and $\mat{P}_{0|0}$ and is computed recursively in two steps. First, the estimate for the next timestep is predicted without new measurement information, known as the \textit{prediction} step, and is given by
\begin{equation}\label{eq:prelims:kf_est_predict}
    \hat{\vec{x}}_{k|k-1} = \mat{F}_k \hat{\vec{x}}_{k-1|k-1}
\end{equation}
and
\begin{equation}\label{eq:prelims:kf_cov_predict}
    \mat{P}_{k|k-1} = \mat{F}_k \mat{P}_{k-1|k-1} \mat{F}_k^\top + \mat{Q}_k\,.
\end{equation}
Next, this prediction is updated with current measurement information, known as the \textit{update} step, and given by
\begin{equation}\label{eq:prelims:kf_est_update}
    \hat{\vec{x}}_{k|k} = \hat{\vec{x}}_{k|k-1}+\mat{P}_{k|k-1}\mat{H}_k^\top\left(\mat{H}_k\mat{P}_{k|k-1}\mat{H}_k^\top + \mat{R}_k\right)^{-1}\left(z_k-\mat{H}_k\hat{\vec{x}}_{k|k-1}\right)
\end{equation}
and
\begin{equation}\label{eq:prelims:kf_cov_update}
    \mat{P}_{k|k} = \mat{P}_{k|k-1}-\mat{P}_{k|k-1}\mat{H}_k^\top\left(\mat{H}_k\mat{P}_{k|k-1}\mat{H}_k^\top + \mat{R}_k\right)^{-1}\mat{H}\mat{P}_{k|k-1}\,.
\end{equation}
In addition to alternating prediction and update steps as time progresses, the update step \eqref{eq:prelims:kf_est_update} and \eqref{eq:prelims:kf_cov_update} can be skipped at timesteps when no measurements are available. Similarly, when multiple independent measurements are present at the same timestep, the update step can be repeated for each measurement individually. Detailed derivations of the KF and discussions on its properties can be found in [huag+chap].

% 
% ##    ## ########     #######  ########  ######## 
% ##   ##  ##          ##     ## ##     ##    ##    
% ##  ##   ##          ##     ## ##     ##    ##    
% #####    ######      ##     ## ########     ##    
% ##  ##   ##          ##     ## ##           ##    
% ##   ##  ##          ##     ## ##           ##    
% ##    ## ##           #######  ##           ##    
% 

\subsection{Kalman Filter Optimality}\label{subsec:prelims:kf_opt}
One of the reasons for the ubiquity and popularity of the KF introduced in section \ref{subsec:prelims:kf} is its optimality in terms of mean square error (MSE) [huag+chap]. That is, the estimate's error covariances, defined by the expectation capturing mean square error,
\begin{equation}
    \mat{P}_{k|k} = \mathbb{E}\left\{\left(\vec{x}_k - \hat{\vec{x}}_{k|k}\right)\left(\vec{x}_k - \hat{\vec{x}}_{k|k}\right)^\top\right\}\,,
\end{equation}
and computed by \eqref{eq:prelims:kf_cov_predict} and \eqref{eq:prelims:kf_cov_update}, can be shown to equal the theoretical lower bound on the covariance of an unbiased estimator when process and measurement models \eqref{eq:prelims:lin_gauss_process_model} and \eqref{eq:prelims:lin_gauss_measurement_model}, respectively, capture the estimated environment exactly [huag refs for crlb]. This property will be used in later cryptographic discussions in this thesis to guarantee estimator performances in terms of MSE. Further reading on the definitions and proofs of KF optimality can be found in [crlb,huag,etc].

% 
% ######## ##    ## ######## 
% ##       ##   ##  ##       
% ##       ##  ##   ##       
% ######   #####    ######   
% ##       ##  ##   ##       
% ##       ##   ##  ##       
% ######## ##    ## ##       
% 

\subsection{Extended Kalman Filter}\label{subsec:prelims:ekf}
The extended Kalman filter (EKF) is a recursive state estimation filter applicable to non-linear models and closely related to the linear KF [ekf paper,huag+chap]. The filter produces estimates and their covariances at each timestep by linearising models at the current estimate and evaluating the filter similarly to the KF. As in the KF, a true state $\vec{x}_k$ is assumed to follow known models. The process model is now non-linear and given by
\begin{equation}\label{eq:prelims:nonlin_gauss_process_model}
    \vec{x}_k = f_k(\vec{x}_{k-1}) + \vec{w}_k\,,
\end{equation}
where again $\vec{w}_k \sim \mathcal{N}(\vec{0}, \mat{Q}_k)$ with known covariance $\mat{Q}_k$. Similarly, measurements are assumed to follow the non-linear measurement model
\begin{equation}\label{eq:prelims:nonlin_gauss_measurement_model}
    \vec{z}_k = h_k(\vec{x}_k) + \vec{v}_k\,,
\end{equation}
with $\vec{v}_k \sim \mathcal{N}(\vec{0}, \mat{R}_k)$ and known covariance $\mat{R}_k$. The EKF \textit{prediction} step is given by
\begin{equation}\label{eq:prelims:ekf_est_predict}
    \hat{\vec{x}}_{k|k-1} = f_k\left(\hat{\vec{x}}_{k-1|k-1}\right)
\end{equation}
and
\begin{equation}\label{eq:prelims:ekf_cov_predict}
    \mat{P}_{k|k-1} = \hat{\mat{F}}_k \mat{P}_{k-1|k-1} \hat{\mat{F}}_k^\top + \mat{Q}_k\,,
\end{equation}
with Jacobian
\begin{equation}\label{eq:prelims:ekf_predict_jacobian}
    \hat{\mat{F}}_k = \left.\frac{\partial f_k}{\partial \vec{x}}\right|_{\hat{\vec{x}}_{k-1|k-1}}
\end{equation}
linearising the process model at the latest estimate for estimate error covariance prediction. The EKF \textit{update} step is given by
\begin{equation}\label{eq:prelims:ekf_est_update}
    \hat{\vec{x}}_{k|k} = \hat{\vec{x}}_{k|k-1}+\mat{P}_{k|k-1}\hat{\mat{H}}_k^\top\left(\hat{\mat{H}}_k\mat{P}_{k|k-1}\hat{\mat{H}}_k^\top + \mat{R}_k\right)^{-1}\left(z_k-h_k(\hat{\vec{x}}_{k|k-1})\right)
\end{equation}
and
\begin{equation}\label{eq:prelims:ekf_cov_update}
    \mat{P}_{k|k} = \mat{P}_{k|k-1}-\mat{P}_{k|k-1}\hat{\mat{H}}_k^\top\left(\hat{\mat{H}}_k\mat{P}_{k|k-1}\hat{\mat{H}}_k^\top + \mat{R}_k\right)^{-1}\hat{\mat{H}}\mat{P}_{k|k-1}\,,
\end{equation}
with Jacobian
\begin{equation}\label{eq:prelims:ekf_update_jacobian}
    \hat{\mat{H}}_k = \left.\frac{\partial h_k}{\partial \vec{x}}\right|_{\hat{\vec{x}}_{k|k-1}}
\end{equation}
linearising the measurement model. Unlike the linear KF, by linearising the models the EKF propagates Gaussian model noises in its estimates that may not be Gaussian in reality, even when process and measurement models \eqref{eq:prelims:nonlin_gauss_process_model} and \eqref{eq:prelims:nonlin_gauss_measurement_model}, respectively, are exactly correct. For this reason, the EKF does not hold the same guarantees on optimality as the KF does. To a similar effect, highly non-linear models or inaccurate models can lead to greater inaccuracies and divergence of estimates from true states in EKF estimates. Despite these downsides, its scalability and efficiency have made the EKF an industry-standard estimation filter for non-linear systems [smth on uses of ekf]. More details on the EKF and its derivation can be found in [huag+chap].

% 
% #### ######## 
%  ##  ##       
%  ##  ##       
%  ##  ######   
%  ##  ##       
%  ##  ##       
% #### ##       
% 

\subsection{Information Filter}\label{subsec:prelims:if}
The information filter (IF) is an algebraic reformulation of the KF from section \ref{subsec:prelims:kf} [niehsenInformationFusionBased2002, Mutambara, another]. The information form of the filter simplifies the update step of the filter making it more suitable when multiple independent measurements are present at the same timestep. The key difference of the IF is the storing and propagation of the information vector $\hat{\vec{y}}_{k|k'}$ and information matrix $\mat{Y}_{k|k'}$ rather than the estimate and its error covariance, $\hat{\vec{x}}_{k|k'}$ and $\mat{P}_{k|k'}$, respectively, stored by the KF. When assuming the same linear and Gaussian models \eqref{eq:prelims:lin_gauss_process_model} and \eqref{eq:prelims:lin_gauss_measurement_model}, the information vector and matrix are related to the estimate and its covariance by
\begin{equation}\label{eq:prelims:info_vec_defn}
    \hat{\vec{y}}_{k|k'} = \mat{P}_{k|k'}^{-1}\hat{\vec{x}}_{k|k'}
\end{equation}
and
\begin{equation}\label{eq:prelims:info_mat_defn}
    \mat{Y}_{k|k'} = \mat{P}_{k|k'}^{-1}\,,
\end{equation}
for an estimated timestep $k$ and measurements from timesteps up to and including $k'$. The estimation of the information vector and information matrix requires an initialisation of $\hat{\vec{y}}_{0|0}$ and $\mat{Y}_{0|0}$, similarly to the KF, and is also performed by iterating distinct predict and update filter steps. The prediction step is given by
\begin{equation}\label{eq:prelims:if_info_vec_predict}
    \hat{\vec{y}}_{k|k-1} = \mat{Y}_{k|k-1}\mat{F}_k\mat{Y}_{k-1|k-1}^{-1}\hat{\vec{y}}_{k-1|k-1}
\end{equation}
and
\begin{equation}\label{eq:prelims:if_info_mat_predict}
    \mat{Y}_{k|k-1} = \left(\mat{F}_k\mat{Y}_{k-1|k-1}^{-1}\mat{F}_k^\top + \mat{Q}_k\right)^{-1}\,.
\end{equation}
The update step is given by
\begin{equation}\label{eq:prelims:if_info_vec_update_single}
    \hat{\vec{y}}_{k|k} = \hat{\vec{y}}_{k|k-1} + \vec{i}_k
\end{equation}
and
\begin{equation}\label{eq:prelims:if_info_mat_update_single}
    \mat{Y}_{k|k} = \mat{Y}_{k|k-1} + \mat{I}_k\,,
\end{equation}
where added terms $\vec{i}_k$ and $\mat{I}_k$ are known as the measurement vector and measurement matrix, respectively, and are defined as
\begin{equation}
    \vec{i}_k = \mat{H}_k^\top\mat{R}_k^{-1}z_k
\end{equation}
and
\begin{equation}
    \mat{I}_k = \mat{H}_k^\top\mat{R}_k^{-1}\mat{H}_k\,.
\end{equation}

Since all information related to a measurement and its sensor are captured in $\vec{i}_k$ and $\mat{I}_k$, namely the measured value $z_k$, measurement model $\mat{H}_k$ and measurement error $\mat{R}_k$, sequential IF update steps required in the presence of multiple sensors are easily computed as a summation of this information from each sensor. That is, if we consider the same process model \eqref{eq:prelims:lin_gauss_process_model} and multiple sensors $i$, $1\leq i\leq n$, making independent measurements that follow models
\begin{equation}
    \vec{z}_{k,i} = \mat{H}_{k,i} \vec{x}_k + \vec{v}_{k,i}\,,
\end{equation}
with $\vec{v}_{k,i}\sim\mathcal{N}(\vec{0},\mat{R}_{k,i})$ and known covariances $\mat{R}_{k,i}$, the update step of the filter using all measurements at timestep $k$ can be written as
\begin{equation}\label{eq:prelims:if_info_vec_update}
    \hat{\vec{y}}_{k|k} = \hat{\vec{y}}_{k|k-1} + \sum_{i=1}^n\vec{i}_{k,i}
\end{equation}
and
\begin{equation}\label{eq:prelims:if_info_mat_update}
    \mat{Y}_{k|k} = \mat{Y}_{k|k-1} + \sum_{i=1}^n\mat{I}_{k,i}\,,
\end{equation}
where information vectors $\vec{i}_{k,i}$ and information matrices $\mat{I}_{k,i}$ are now dependent on sensor $i$ and given by
\begin{equation}\label{eq:prelims:if_measurement_vec_defn}
    \vec{i}_{k,i} = \mat{H}_{k,i}^\top\mat{R}_{k,i}^{-1}z_{k,i}
\end{equation}
and
\begin{equation}\label{eq:prelims:if_measurement_mat_defn}
    \mat{I}_{k,i} = \mat{H}_{k,i}^\top\mat{R}_{k,i}^{-1}\mat{H}_{k,i}\,.
\end{equation}
The easily computed summation has led to the IF being particularly suited to distributed estimation environments, where multiple sensors are present and communicational costs need to be reduced [if egs]. In addition, since the IF is strictly a rearrangement of terms in the KF, it holds the same optimality properties as the KF, described in section \ref{subsec:prelims:kf_opt}. For additional reading on the IF, see [mutambara+chap].

% 
% ######## #### ######## 
% ##        ##  ##       
% ##        ##  ##       
% ######    ##  ######   
% ##        ##  ##       
% ##        ##  ##       
% ######## #### ##       
% 

\subsection{Extended Information Filter}\label{subsec:prelims:eif}
The extended information filter (EIF) is an algebraic reformulation of the EKF and represents a non-linear model extension to the linear IF [thrun, another]. As with the IF, its simplification of the update step to a trivial sum has led to its adoption in suitable distributed environments with multiple sensors and a desire to reduce communicational costs [tobias garritsen thesis]. Similarly, the EIF estimates and propagates the information vector $\hat{\vec{y}}_{k|k'}$ and information matrix $\mat{Y}_{k|k'}$ that relate to the state estimate and its covariance by \eqref{eq:prelims:info_vec_defn} and \eqref{eq:prelims:info_mat_defn}, respectively. Assuming the non-linear process model \eqref{eq:prelims:nonlin_gauss_process_model} and multiple sensors $i$, $1\leq i \leq n$, making independent non-linear measurements that follow models
\begin{equation}
    \vec{z}_{k,i} = h_{k,i}\left(\vec{x}_k\right) + \vec{v}_{k,i}\,,
\end{equation}
with $\vec{v}_{k,i}\sim\mathcal{N}(\vec{0},\mat{R}_{k,i})$ and known covariances $\mat{R}_{k,i}$, the EIF predict step is given by
\begin{equation}\label{eq:prelims:eif_info_vec_predict}
    \hat{\vec{y}}_{k|k-1} = \mat{Y}_{k|k-1}f_k\left(\mat{Y}_{k-1|k-1}^{-1}\hat{\vec{y}}_{k-1|k-1}\right)
\end{equation}
and
\begin{equation}\label{eq:prelims:eif_info_mat_predict}
    \mat{Y}_{k|k-1} = \left(\hat{\mat{F}}_k\mat{Y}_{k-1|k-1}^{-1}\hat{\mat{F}}_k^\top + \mat{Q}_k\right)^{-1}\,,
\end{equation}
with Jacobian linearising the process model
\begin{equation}
    \hat{\mat{F}}_k = \left.\frac{\partial f_k}{\partial \vec{x}}\right|_{\hat{\vec{x}}_{k-1|k-1}}\,.
\end{equation}
The update step of the filter using all $n$ measurements is given by
\begin{equation}\label{eq:prelims:eif_info_vec_update}
    \hat{\vec{y}}_{k|k} = \hat{\vec{y}}_{k|k-1} + \sum_{i=1}^n\vec{i}_{k,i}
\end{equation}
and
\begin{equation}\label{eq:prelims:eif_info_mat_update}
    \mat{Y}_{k|k} = \mat{Y}_{k|k-1} + \sum_{i=1}^n\mat{I}_{k,i}\,,
\end{equation}
where information vectors $\vec{i}_{k,i}$ and information matrices $\mat{I}_{k,i}$ now linearise the measurement model and are given by
\begin{equation}\label{eq:prelims:eif_measurement_vec_defn}
    \vec{i}_{k,i} = \hat{\mat{H}}_{k,i}^\top\mat{R}_{k,i}^{-1}\left(z_{k,i} - h_{k,i}\left(\mat{Y}_{k|k-1}^{-1}\hat{\vec{y}}_{k|k-1}\right) + \hat{\mat{H}}_{k,i}\mat{Y}_{k|k-1}^{-1}\hat{\vec{y}}_{k|k-1}\right)
\end{equation}
and
\begin{equation}\label{eq:prelims:eif_measurement_mat_defn}
    \mat{I}_{k,i} = \hat{\mat{H}}_{k,i}^\top\mat{R}_{k,i}^{-1}\hat{\mat{H}}_{k,i}\,,
\end{equation}
with Jacobian
\begin{equation}
    \hat{\mat{H}}_{k,i} = \left.\frac{\partial h_{k,i}}{\partial \vec{x}}\right|_{\hat{\vec{x}}_{k|k-1}}\,.
\end{equation}
Similarly to the EKF, the linearisation of models leads to estimation errors making optimality guarantees of the KF and IF not hold for the EIF. For further reading and applications of the EIF, see [refs].

% 
% ########  ######  #### 
% ##       ##    ##  ##  
% ##       ##        ##  
% ######   ##        ##  
% ##       ##        ##  
% ##       ##    ##  ##  
% ##        ######  #### 
% 

\subsection{Covariance Intersection and Fast Covariance Intersection}\label{subsec:prelims:fci}
In the filters presented in the previous sections, measurements from the same timestep $k$ must be independent for sequential update steps to fuse them correctly. That is, non-zero cross-correlations between measurements can lead to overly confident estimate error covariances and estimation track divergence [crosscor paper]. In some cases, this independence of measurements cannot be guaranteed and a conservative fusion method is required to obtain a final estimate and its error covariance using all available measurements. A typical scenario is the fusion of local estimator estimates themselves, such as those produced by separate KF or EKF instances, that may contain cross-correlations due to shared process model assumptions during estimation [ci and dependence ref]. Covariance intersection (CI), is a conservative fusion algorithm that produces such a fused estimate and its error covariance from estimates and their error covariances (or measurements with known error covariances) with unknown cross-correlations [julierNondivergentEstimation]. The CI fusion of $n$ estimates $\hat{\vec{x}}_i$, $1\leq i \leq n$, and their error covariances $\mat{P}_i$, $1\leq i \leq n$, produces a fused estimate $\hat{\vec{x}}_{\mathsf{fus}}$ and its error covariance $\mat{P}_{\mathsf{fus}}$, and is given by
\begin{equation}\label{eq:prelims:ci_est}
    \hat{\vec{x}}_{\mathsf{fus}} = \mat{P}_{\mathsf{fus}}\sum_{i=1}^n \omega_i \mat{P}_i^{-1} \hat{\vec{x}}_i
\end{equation}
and
\begin{equation}\label{eq:prelims:ci_cov}
\mat{P}_{\mathsf{fus}}=\left(\sum_{i=1}^{n}\omega_i \mat{P}_i^{-1}\right)^{-1}\,,
\end{equation}
for some weights $0\leq \omega_i \leq 1$, $1\leq i \leq n$, such that
\begin{equation}
    \sum_{i=1}^n \omega_i = 1\,.
\end{equation}



They are chosen in a way to speed up convergence and minimize the error by minimizing a certain specified property of the resulting fused estimate covariance. One such property, the covariance trace, requires the solution to
\begin{equation}\label{eq:prelims:ci_min_trace}
   \argmin_{\omega_1,\dots,\omega_n} \{\tr(\mP)\}\! =\! \argmin_{\omega_1,\dots,\omega_n} \left\{\tr\left(\!\left(\sum_{i=1}^{n}\omega_i \mP_i^{-1}\!\right)^{-1}\right)\!\right\} 
\end{equation}
for computing weights $\omega_i$. However, minimizing this non-linear cost function can be very computationally costly and has led to the development of faster approximation techniques.


note the form of info filters


The Fast Covariance Intersection (FCI) algorithm from [niehsenInformationFusionBased2002] is a non-iterative method for approximating the solution to \eqref{eq:prelims:ci_min_trace} without the loss of guaranteed consistency. It is computed by defining a new constraint
\begin{equation}
   \omega_i \tr(\mP_i) - \omega_j \tr(\mP_j) = 0,\ i,j=1,2,\dots,n \label{eqn:fci_eq_big}
\end{equation}
on $\omega_i$ and solving the resulting equations instead. In the two sensor case, this results in the solving of
\begin{equation}
   \omega_1 \tr(\mP_1) - \omega_2 \tr(\mP_2) = 0,\ \omega_1 + \omega_2 = 1\enspace. \label{eqn:fci_2sen_omega_sum_eq}
\end{equation}
When computed for $n$ sensors, the highly redundant \eqref{eqn:fci_eq_big} can have its largest linearly independent subset represented by
\begin{equation}
   \omega_i \tr(\mP_i) - \omega_{i+1} \tr(\mP_{i+1}) = 0,\ i=1,2,\dots,n-1\enspace, \label{eqn:fci_eq}
\end{equation}
and requires the solution to the linear problem
\begin{equation}
   \begingroup
   \setlength\arraycolsep{2pt}
   \begin{bmatrix}
      \mathcal{P}_1 & -\mathcal{P}_2 & 0 & \cdots & 0 \\
      \cdots & \cdots & \cdots & \cdots & \cdots & \\
      0 & \cdots & 0 & \mathcal{P}_{n-1} & -\mathcal{P}_{n} \\
      1 & \cdots & 1 & 1 & 1 &
   \end{bmatrix}
   \begin{bmatrix}
      \omega_1 \\
      \vdots \\
      \omega_{n-1} \\
      \omega_{n}
   \end{bmatrix}
   =
   \begin{bmatrix}
      0 \\
      \vdots \\
      0 \\
      1
   \end{bmatrix}\enspace, \label{eqn:fci_eq_sys}
   \endgroup
\end{equation}
where we let $\mathcal{P}_i = \tr(\mP_i)$.

Our proposed filter aims to solve FCI fusion, namely () and (), using only encrypted values from each sensor $i$, and leaking only the weight values $\omega_1,\dots,\omega_n$.

% 
%  .d8888b.  8888888888 .d8888b.       8888888b.  8888888b.  8888888888 888      8888888 888b     d888 
% d88P  Y88b 888       d88P  Y88b      888   Y88b 888   Y88b 888        888        888   8888b   d8888 
% Y88b.      888       888    888      888    888 888    888 888        888        888   88888b.d88888 
%  "Y888b.   8888888   888             888   d88P 888   d88P 8888888    888        888   888Y88888P888 
%     "Y88b. 888       888             8888888P"  8888888P"  888        888        888   888 Y888P 888 
%       "888 888       888    888      888        888 T88b   888        888        888   888  Y8P  888 
% Y88b  d88P 888       Y88b  d88P      888        888  T88b  888        888        888   888   "   888 
%  "Y8888P"  8888888888 "Y8888P"       888        888   T88b 8888888888 88888888 8888888 888       888 
%                                                                                                      
%                                                                                                      
%                                                                                                      
% 

\section{Encryption Preliminaries}
Used cryptographic notions and schemes are summarised here. In addition, the encoding of floating point numbers to integers suitable for encryption by the presented schemes is introduced as well.

% 
% ##    ##  #######  ######## ####  #######  ##    ##  ######  
% ###   ## ##     ##    ##     ##  ##     ## ###   ## ##    ## 
% ####  ## ##     ##    ##     ##  ##     ## ####  ## ##       
% ## ## ## ##     ##    ##     ##  ##     ## ## ## ##  ######  
% ##  #### ##     ##    ##     ##  ##     ## ##  ####       ## 
% ##   ### ##     ##    ##     ##  ##     ## ##   ### ##    ## 
% ##    ##  #######     ##    ####  #######  ##    ##  ######  
% 

\subsection{Meeting Cryptographic Notions}

% 
% ########  ##     ## ######## 
% ##     ## ##     ## ##       
% ##     ## ##     ## ##       
% ########  ######### ######   
% ##        ##     ## ##       
% ##        ##     ## ##       
% ##        ##     ## ######## 
% 

\subsection{Paillier Homomorphic Encryption Scheme}
The Paillier encryption scheme [paillierPublicKeyCryptosystemsBased1999] is an additively homomorphic encryption scheme that bases its security on the decisional composite residuosity assumption (DCRA) and meets the security notion of IND-CPA. Key generation of the Paillier scheme is performed by choosing two sufficiently large primes $p$ and $q$, and computing $N=pq$. A generator $g$ is also required for encryption, which is often set to $g=N+1$ when $p$ and $q$ are of equal bit length [katzIntroductionModernCryptography2008]. The public key is defined by $(N, g)$ and the secret key by $(p, q)$.

Encryption of a plaintext message $a \in \mathbb{Z}_N$, producing ciphertext $c \in \mathbb{Z}^{*}_{N^2}$, is computed by
\begin{equation}
    c = g^a \rho^N \pmod{N^2}
\end{equation}
for a randomly chosen $\rho \in \mathbb{Z}_{N}$. Here, $\rho^N$ can be considered the noise term which hides the value $g^a \pmod{N^2}$, which due to the scheme construction, is an easily computable discrete logarithm. The decryption of a ciphertext is computed by
\begin{equation}
    a = \frac{L(c^\lambda\pmod{N^2})}{L(g^\lambda\pmod{N^2})} \pmod{N}
\end{equation}
where $\lambda = \mathsf{lcm}(p-1, q-1)$ and $L(\psi) = \frac{\psi-1}{N}$.

In addition to encryption and decryption, the following homomorphic functions are provided by the Paillier scheme. $\forall a_1,a_2 \in \mathbb{Z}_N$,
\begin{align}
    \mathcal{D}(\mathcal{E}(a_1)\mathcal{E}(a_2) \hspace{-7pt} \pmod{N^2}) &= a_1+a_2 \hspace{-7pt} \pmod{N}\,, \label{eqn:paillier_hom_add}\\
    \mathcal{D}(\mathcal{E}(a_1)g^{a_2} \hspace{-7pt} \pmod{N^2}) &= a_1+a_2\hspace{-7pt}\pmod{N}\,, \label{eqn:paillier_hom_plain_add}\\
    \mathcal{D}(\mathcal{E}(a_1)^{a_2} \hspace{-7pt} \pmod{N^2}) &= a_1a_2 \hspace{-7pt} \pmod{N}\,. \label{eqn:paillier_hom_mult}
\end{align}

% 
%    ###     ######    ######   
%   ## ##   ##    ##  ##    ##  
%  ##   ##  ##        ##        
% ##     ## ##   #### ##   #### 
% ######### ##    ##  ##    ##  
% ##     ## ##    ##  ##    ##  
% ##     ##  ######    ######   
% 

\subsection{Joye-Libert Aggregation Scheme}
The Joye-Libert privacy-preserving aggregation scheme [joyeScalableSchemePrivacyPreserving2013] is a scheme defined on time-series data and meets the security notion of Aggregator Obliviousness (AO) [shiPrivacyPreservingAggregationTimeSeries2011]. Similarly to the Paillier scheme, it bases its security on the DCRA. A notable difference to a public-key encryption scheme is its need for a trusted party to perform the initial key generation and distribution.

Key generation is computed by first choosing two equal-length and sufficiently large primes $p$ and $q$, and computing $N=pq$. A hash function $H:\mathbb{Z} \rightarrow \mathbb{Z}_{N^2}^*$ is defined and the public key is set to $(N, H)$. $n$ private keys are generated by choosing $sk_i,\,i\in\{1,\dots,n\}$, uniformly from $\mathbb{Z}_{N^2}$ and distributing them to $n$ participants (whose values are to be aggregated), while the last key is set as
\begin{equation}
    sk_0 = -\sum^{n}_{i=1}sk_i\,,
\end{equation}
and sent to the aggregator.

Encryption of plaintext $a^{(t)}_{i} \in \mathbb{Z}_N$ to ciphertext $c^{(t)}_{i} \in \mathbb{Z}_{N^2}$ at instance $t$ is computed by user $i$ as
\begin{equation}
    c^{(t)}_{i} = (N+1)^{a^{(t)}_{i}} H(t)^{sk_i} \pmod{N^2}\,.
\end{equation}
Here, we can consider $H(t)^{sk_i}$ the noise term which hides the easily computable discrete logarithm $g^{a^{(t)}_{i}} \pmod{N^2}$, where $g=N+1$ (as with the Paillier scheme above).

When all encryptions $c^{(t)}_{i},\,i\in\{1,\dots,n\}$ are sent to the aggregator, summation and decryption of the aggregated sum are computed by the functions
\begin{equation}
    c^{(t)} = H(t)^{sk_0}\prod^{n}_{i=1}c^{(t)}_{i} \pmod{N^2} \label{eqn:agg_summation}
\end{equation}
and
\begin{equation}
    \sum^{n}_{i=1}a^{(t)}_{i} = \frac{c^{(t)}-1}{N} \pmod{N}\,. \label{eqn:agg_decryption}
\end{equation}
Correctness follows from $\sum^{n}_{i=0}sk_i = 0$, and thus
\begin{equation*}
    \begin{split}
        &H(t)^{sk_0}\prod^{n}_{i=1}c^{(t)}_{i} \pmod{N^2} \\
        \equiv &H(t)^{sk_0}\prod^{n}_{i=1}(N+1)^{a^{(t)}_{i}} H(t)^{sk_i} \pmod{N^2} \\
        \equiv &H(t)^{\sum^n_{j=0}sk_j} \prod^{n}_{i=1}(N+1)^{a^{(t)}_{i}} \pmod{N^2} \\
        \equiv &(N+1)^{\sum^n_{i=1}a^{(t)}_{i}} \pmod{N^2}\,,
    \end{split}
\end{equation*}
removing all noise terms.

% 
%  #######  ########  ########  ######## ########  
% ##     ## ##     ## ##     ## ##       ##     ## 
% ##     ## ##     ## ##     ## ##       ##     ## 
% ##     ## ########  ##     ## ######   ########  
% ##     ## ##   ##   ##     ## ##       ##   ##   
% ##     ## ##    ##  ##     ## ##       ##    ##  
%  #######  ##     ## ########  ######## ##     ## 
% 

\subsection{Lewi Order-Revealing Encryption Scheme}
For ORE, we use the Lewi symmetric-key Left-Right ORE scheme as it has the added property of only allowing certain comparisons between cyphertexts. This property can be used to decide which values may not be compared, which will be shown in the section ... It is described as follows: two encryption functions allow integers to be encrypted as either a ``Left'' ($L$) or ``Right'' ($R$) encryption by
\begin{equation}
   \begin{aligned} \label{eqn:lewi_l_r}
      &\operatorfont{enc}^L_{ORE}(k, x) = \mathcal{E}^L_{ORE,k}(x)\enspace, \\
      &\operatorfont{enc}^R_{ORE}(k, y) = \mathcal{E}^R_{ORE,k}(y)\enspace,
   \end{aligned}
\end{equation}
and only comparisons between an $L$ and an $R$ encryption are possible, by
\begin{equation}
   \operatorfont{cmp}_{ORE}(\mathcal{E}^L_{ORE}(x),\ \mathcal{E}^R_{ORE}(y)) = \operatorfont{cmp}(x, y)\enspace. \label{eqn:lewi_cmp}
\end{equation}
Note that no decryption function is provided as only encryptions are required to provide a secure comparison. The Lewi ORE encryption scheme provides security against the simulation-based security model [chenettePracticalOrderRevealingEncryption2016] but is not secure against the IND-OCPA model.

% 
% ######## ##    ##  ######  
% ##       ###   ## ##    ## 
% ##       ####  ## ##       
% ######   ## ## ## ##       
% ##       ##  #### ##       
% ##       ##   ### ##    ## 
% ######## ##    ##  ######  
% 

\subsection{Encoding Numbers for Encryption}
In both the Paillier and Joye-Libert schemes, as well as the one we introduce, meaningful inputs $a$ are bounded to $a \in \mathbb{Z}_N$. For this reason, real-valued estimation variables require quantisation and integer mapping for encryption and aggregation. We will rely on a generalised Q number encoding [oberstarFixedPointRepresentationFractional2007] due to implementation simplicity and applicability.

We will consider a subset of rational numbers in terms of a range $M \in \mathbb{N}$ and fractional precision $\phi \in \mathbb{N}$. This contrasts with the common definition in terms of total and fractional bits [oberstarFixedPointRepresentationFractional2007, schulzedarupEncryptedCooperativeControl2019, farokhiSecurePrivateControl2017], but allows for a direct mapping to integer ranges which are not a power of two. A rational subset $\mathbb{Q}_{M,\phi}$ is then given by
\begin{equation}
    \mathbb{Q}_{M,\phi} = \left\{o \,\middle|\, \phi o \in \mathbb{N} \wedge -\left\lfloor\frac{M}{2}\right\rfloor \leq \phi o < \left\lfloor\frac{M}{2}\right\rfloor \right\}\,,
\end{equation}
and we can quantize any real number $a$ by taking the nearest rational $o \in \mathbb{Q}_{M,\phi}$, that is, $\argmin_{o\in\mathbb{Q}_{M,\phi}} |a-o|$. In this form, mapping rationals $\mathbb{Q}_{M,\phi}$ to an encryption range $\mathbb{Z}_N$ is achieved by choosing $M=N$ and handling negatives by modulo arithmetic. Additionally, we note that the Q number format requires a precision factor $\phi$ to be removed after each encoded multiplication. This is captured by a third parameter $d$; the number of additional precision factors present in encodings.

The function for \textit{combined} quantisation and encoding, $\mathsf{E}_{M,\phi,d}(a)$, of a given number $a \in \mathbb{R}$ and with an integer range $\mathbb{Z}_M$, precision $\phi$ and scaling for $d$ prior encoded multiplications is given by
\begin{equation}
    \mathsf{E}_{M,\phi,d}(a) = \left\lfloor \phi^{d+1} a \right\rceil \pmod{M}\,. \label{eqn:encode}
\end{equation}
Decoding of an integer $u \in \mathbb{Z}_M$, is given by
\begin{equation}
    \mathsf{E}^{-1}_{M,\phi,d}(u) \!=\! 
    \begin{dcases}
        \frac{u\hspace{-8pt}\pmod{M}}{\phi^{d+1}}, &\hspace{-6pt}u\hspace{-8pt}\pmod{M} \leq \left\lfloor\frac{M}{2}\right\rfloor \\
        -\frac{M - u\hspace{-8pt}\pmod{M}}{\phi^{d+1}}, &\hspace{-6pt}\text{otherwise} \\
    \end{dcases}\!\!.\label{eqn:decode}
\end{equation}

This encoding scheme provides the following homomorphic operations,
\begin{equation}
    \begin{split}
        \mathsf{E}_{M,\phi,d}(a_1) + \mathsf{E}_{M,\phi,d}(a_2)& \pmod{M} =\\
        &\mathsf{E}_{M,\phi,d}(a_1+a_2)
    \end{split}\label{eqn:encoding_homomorphic_add}
\end{equation}
and
\begin{equation}
    \begin{split}
        \mathsf{E}_{M,\phi,d}(a_1)\mathsf{E}_{M,\phi,d}(a_2)& \pmod{M} =\\
        &\mathsf{E}_{M,\phi,d+1}(a_1a_2)\,,
    \end{split}
\end{equation}
noting that when $M=N$, the operations and modulus correspond with those in the Paillier homomorphic operations \eqref{eqn:paillier_hom_add}, \eqref{eqn:paillier_hom_plain_add} and \eqref{eqn:paillier_hom_mult}, and the Joye-Libert sum \eqref{eqn:agg_decryption}.

In general, the choice of a large precision parameter $\phi$ may reduce quantisation errors introduced in \eqref{eqn:encode}, but risks overflow after too many multiplications. Given the largest number of encoded multiplications, $d_{max}$, and the largest value to be encoded $a_{max}$, the parameter should be chosen such that
\begin{equation}
    \left|\phi^{d_{max}+1}a_{max}\right| < \left\lfloor \frac{M}{2} \right\rfloor\,.
\end{equation}
In practice, $N$ is typically very large ($N>2^{1024}$) and this condition can be ignored when $M=N$, as $\phi$ can be made sufficiently large to make quantisation errors negligible.